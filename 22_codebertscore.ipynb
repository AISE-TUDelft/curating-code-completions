{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, json, pandas as pd, matplotlib.pyplot as plt, datetime as dt, torch\n",
    "while not os.getcwd().endswith('-analysis'): os.chdir('..')\n",
    "\n",
    "SERVER_DATA_DIR = 'REDACTED' \n",
    "DATA_DIR = os.path.join('..', 'data')\n",
    "\n",
    "# NOTE: do not sync \n",
    "# _ = subprocess.run(['rsync', '-avz', f'larisa:{SERVER_DATA_DIR}', DATA_DIR])\n",
    "DATA_DIR = os.path.join(DATA_DIR, SERVER_DATA_DIR.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running CodeBERTScore on Prediction & Ground-Truth per Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import code_bert_score \n",
    "import datetime as dt, json\n",
    "\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class Query:\n",
    "\n",
    "    user            : str\n",
    "    prefix          : str\n",
    "    suffix          : str\n",
    "    trigger         : str\n",
    "    language        : str\n",
    "    ide             : str\n",
    "    version         : str\n",
    "    store           : bool\n",
    "    timestamp       : dt.datetime\n",
    "    predictions     : dict\n",
    "    predict_time    : float\n",
    "    survey          : bool\n",
    "\n",
    "    time_since_last_completion: float\n",
    "    filter_type     : str\n",
    "    filter_time     : float\n",
    "    should_filter   : bool\n",
    "    study_version   : str\n",
    "\n",
    "    @classmethod \n",
    "    def from_dict(cls, data: dict) -> 'Query':\n",
    "        ''' Parse json dict into pythonic types, returning Query or VerifiedQuery '''\n",
    "\n",
    "        try: \n",
    "            data['timestamp'] = dt.datetime.fromisoformat(data['timestamp'])\n",
    "            return VerifiedQuery.from_dict(data) \\\n",
    "                if 'verifyToken' in data else cls(**data)\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing query: {e}')\n",
    "            pprint(data)\n",
    "\n",
    "@dataclass \n",
    "class VerifiedQuery(Query):\n",
    "    verify_token    : str\n",
    "    chosen_model    : str\n",
    "    ground_truth    : str\n",
    "    shown_times     : Optional[list[dt.datetime]]\n",
    "    accept_time     : dt.datetime\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict):\n",
    "        ''' parse json dict into pythonic types '''\n",
    "\n",
    "        data['verify_token'] = data.pop('verifyToken')\n",
    "        data['accept_time'] = dt.datetime.fromisoformat(data['accept_time'])\n",
    "        \n",
    "        # pushed bugfix for shown_times field on client-side\n",
    "        data['shown_times'] = [dt.datetime.fromisoformat(t) for t in data['shown_times']] \\\n",
    "            if 'shown_times' in data else None\n",
    "\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    64,870 total (valid) queries\n",
      "\n",
      "    498 verified queries\n",
      "    64,372 unverified queries\n",
      "\n",
      "    387 with ground truth\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "unfiltered_queries_per_user = {\n",
    "    user: sorted([\n",
    "        Query.from_dict({\n",
    "            **json.load(open(os.path.join(DATA_DIR, user, query))),\n",
    "            'user': user\n",
    "        }) for query in os.listdir(os.path.join(DATA_DIR, user))\n",
    "    ], key=lambda q: q.timestamp, reverse=True) \n",
    "    for user in os.listdir(DATA_DIR) if user != '.DS_Store'\n",
    "}\n",
    "\n",
    "oops_date = dt.datetime(2024, 3, 21, 17, 17, 50)\n",
    "queries_per_user = {\n",
    "    user: [q for q in queries if q.timestamp >= oops_date] \\\n",
    "    for user, queries in unfiltered_queries_per_user.items()\n",
    "}\n",
    "\n",
    "queries = [q for user_queries in queries_per_user.values() for q in user_queries]\n",
    "del queries_per_user, unfiltered_queries_per_user\n",
    "\n",
    "print(f'''\n",
    "    {len(queries):,} total (valid) queries\n",
    "\n",
    "    {len([q for q in queries if isinstance(q, VerifiedQuery)]):,} verified queries\n",
    "    {len([q for q in queries if not isinstance(q, VerifiedQuery)]):,} unverified queries\n",
    "\n",
    "    {len([q for q in queries if hasattr(q, 'ground_truth') and q.ground_truth])} with ground truth\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_queries = [q for q in queries if hasattr(q, 'ground_truth')]\n",
    "len(gt_queries)\n",
    "del queries\n",
    "\n",
    "# wrong_qs = [q for q in gt_queries if not q.ground_truth] \n",
    "# pprint(wrong_qs[0])\n",
    "\n",
    "# We're just going to assume that the bsc students that implemented this logic did it correctly. \n",
    "# but given my experience, they almost certainly did not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [q.predictions[q.chosen_model] for q in gt_queries]\n",
    "targets = [q.ground_truth for q in gt_queries]\n",
    "langs = [q.language for q in gt_queries]\n",
    "\n",
    "len(preds), len(targets)\n",
    "\n",
    "for q in gt_queries: \n",
    "    if q.ground_truth: continue\n",
    "    # print(q.predictions, q.ground_truth)\n",
    "    if type(q.ground_truth) == None: print('found none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CodeBERTScore\n",
    "We need to sort our prediction/ground_truth on language, as this is what codebertscore expects. \n",
    "The only languages we can use are `python`, `javascript`, `c`, `cpp`, and `java`. \n",
    "\n",
    "- We probably can pool `javascriptreact`, `typescript`, and `typescriptreact` to `javascript`\n",
    "- Also `ipynb` to `python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 236\n",
      "html: 81\n",
      "php: 48\n",
      "latex: 35\n",
      "css: 27\n",
      "javascript: 18\n",
      "markdown: 15\n",
      "typescriptreact: 12\n",
      "typescript: 9\n",
      "java: 4\n",
      "properties: 3\n",
      "jsonc: 2\n",
      "json: 2\n",
      "bicep: 1\n",
      "yaml: 1\n",
      "ignore: 1\n",
      "cpp: 1\n",
      "scminput: 1\n",
      "pip-requirements: 1\n"
     ]
    }
   ],
   "source": [
    "queries_per_language = {lang: [q for q in gt_queries if q.language == lang] for lang in set(langs)}\n",
    "\n",
    "# sort on list length \n",
    "queries_per_language = dict(sorted(queries_per_language.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "for lang, queries in queries_per_language.items():\n",
    "    print(f'{lang}: {len(queries):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 237\n",
      "javascript: 39\n",
      "c: 0\n",
      "cpp: 1\n",
      "java: 4\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Let's merge them into their supposed groups \n",
    "# python < pip-requirements \n",
    "# javascript < typescript, typescriptreact, json, jsonc \n",
    "# and, optionally, php can maybe be merged with typescriptreact due to their similar design\n",
    "\n",
    "def construct_valid_inputs(queries): \n",
    "\n",
    "    valid_langs = {\n",
    "        'python': ['python', 'pip-requirements'], \n",
    "        'javascript': ['javascript', 'typescript', 'typescriptreact', 'php', 'html'],  # TODO: and maybe php & html\n",
    "        'c': ['c'], \n",
    "        'cpp': ['cpp'], \n",
    "        'java': ['java']\n",
    "    }\n",
    "\n",
    "    queries_per_language = {lang: [q for q in queries if q.language in langs] for lang, langs in valid_langs.items()}\n",
    "    return queries_per_language\n",
    "\n",
    "# NOTE: for all filters\n",
    "for lang, queries in construct_valid_inputs(gt_queries).items():\n",
    "    print(f'{lang}: {len(queries):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint_h: 130 \t {'python': 91, 'javascript': 7, 'c': 0, 'cpp': 0, 'java': 0}\n",
      "context: 109 \t {'python': 45, 'javascript': 3, 'c': 0, 'cpp': 1, 'java': 2}\n",
      "feature: 42 \t {'python': 20, 'javascript': 3, 'c': 0, 'cpp': 0, 'java': 0}\n",
      "joint_a: 75 \t {'python': 37, 'javascript': 10, 'c': 0, 'cpp': 0, 'java': 0}\n",
      "no_filter: 138 \t {'python': 42, 'javascript': 16, 'c': 0, 'cpp': 0, 'java': 2}\n"
     ]
    }
   ],
   "source": [
    "# let's divide per filter \n",
    "q_per_filter = {}\n",
    "for q in gt_queries:\n",
    "    if q.ground_truth == None: continue \n",
    "    if q.filter_type not in q_per_filter: q_per_filter[q.filter_type] = []\n",
    "    q_per_filter[q.filter_type].append(q)\n",
    "\n",
    "for filter_type, queries in q_per_filter.items():\n",
    "    lang_counts = {lang: len(v) for lang, v in construct_valid_inputs(queries).items()}\n",
    "    print(f'{filter_type}: {len(queries):,} \\t {lang_counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty gt: PR:={styles.top}>{t('chat')}</span>         \tGT:                                        \n",
      "empty gt: PR:={styles.top}>{t('chat')}</span>         \tGT:                                        \n",
      "empty gt: PR:function createClient(options) {         \tGT:                                        \n",
      "empty gt: PR:ARIFICATION REQUIRED                     \tGT:                                        \n",
      "empty gt: PR:th_accuracy(weights.values()))           \tGT:                                        \n",
      "empty gt: PR:INGO_DECIMAL                             \tGT:                                        \n",
      "empty gt: PR:bd(total)                                \tGT:                                        \n",
      "empty gt: PR:ghts = {issue: Decimal(weights[issue]) for issue in self.domain.getIssues()} \tGT:                                        \n",
      "empty gt: PR:.get(key)                                \tGT:                                        \n",
      "empty gt: PR:Store                                    \tGT:                                        \n",
      "empty gt: PR:'staging'                                \tGT:                                        \n",
      "empty gt: PR:get_openapi(                             \tGT:                                        \n",
      "empty gt: PR:.client.Client from 'pyperclip'          \tGT:                                        \n",
      "empty gt: PR:city, house.state, house.zip, house.country, house.country_code) \tGT:                                        \n",
      "empty gt: PR:from 'http://www.google.com/spreadsheet/sheets/sheets.xls' \tGT:                                        \n",
      "empty gt: PR::                                        \tGT:                                        \n",
      "empty gt: PR:rityError                                \tGT:                                        \n",
      "empty gt: PR:iq=True)                                 \tGT:                                        \n",
      "empty gt: PR:= \"display: block\";                      \tGT:                                        \n",
      "empty gt: PR:);                                       \tGT:                                        \n",
      "empty gt: PR:{                                        \tGT:                                        \n",
      "empty gt: PR:{                                        \tGT:                                        \n",
      "empty gt: PR:ght: 20px;                               \tGT:                                        \n",
      "empty gt: PR:{                                        \tGT:                                        \n"
     ]
    }
   ],
   "source": [
    "pairs = [(q.predictions[q.chosen_model], q.ground_truth) for q in q_per_filter['joint_h']]\n",
    "for pred, gt in pairs: \n",
    "    if pred.strip() == '':  print(f'empty pr: PR:{pred:40} \\tGT:{gt:40}')\n",
    "    if gt.strip() == '':    print(f'empty gt: PR:{pred:40} \\tGT:{gt:40}')\n",
    "    # if pred is not gt: \n",
    "    #     print(f'{pred:40} \\t{gt:40}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(q_per_filter, use_sources=False):\n",
    "    results = {filter_type: {\n",
    "        'precision': torch.tensor([]), \n",
    "        'recall': torch.tensor([]), \n",
    "        'f1': torch.tensor([]), \n",
    "        'f3': torch.tensor([]),\n",
    "        } for filter_type in q_per_filter.keys()}\n",
    "\n",
    "    for filter_type, queries in q_per_filter.items(): \n",
    "        print(f'\\ndoing {filter_type}', end='')\n",
    "        q_per_lang = construct_valid_inputs(queries)\n",
    "\n",
    "        for lang, queries in q_per_lang.items():\n",
    "            print(f'\\t{lang}: {len(queries)}', end='')\n",
    "\n",
    "            if len(queries) == 0: continue\n",
    "            preds   = [q.predictions[q.chosen_model] for q in queries]\n",
    "            gts     = [q.ground_truth for q in queries]\n",
    "\n",
    "            if not use_sources:\n",
    "                precision, recall, f1, f3 = code_bert_score.score(\n",
    "                    preds, gts, lang, no_punc=False, verbose=True, batch_size=1\n",
    "                )\n",
    "            else: \n",
    "                sources = [q.prefix[-7984 // 2:] for q in queries] # let's not kill my computer\n",
    "                precision, recall, f1, f3 = code_bert_score.score(\n",
    "                    preds, gts, lang, verbose=True, sources=sources, no_punc=False, device='mps', \n",
    "                    batch_size=1\n",
    "                )\n",
    "\n",
    "            for metric in results[filter_type].keys(): \n",
    "                results[filter_type][metric] = torch.cat((results[filter_type][metric], locals()[metric]))\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doing joint_h\tpython: 91"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tjavascript: 7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tc: 0\tcpp: 0\tjava: 0\n",
      "doing context\tpython: 45"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tjavascript: 3\tc: 0\tcpp: 1\tjava: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doing feature\tpython: 20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tjavascript: 3\tc: 0\tcpp: 0\tjava: 0\n",
      "doing joint_a\tpython: 37"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tjavascript: 10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tc: 0\tcpp: 0\tjava: 0\n",
      "doing no_filter\tpython: 42"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tjavascript: 16"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tc: 0\tcpp: 0\tjava: 2\n",
      "joint_h   : ['precision 0.75', 'recall 0.74', 'f1 0.74', 'f3 0.74']\n",
      "context   : ['precision 0.77', 'recall 0.76', 'f1 0.77', 'f3 0.76']\n",
      "feature   : ['precision 0.86', 'recall 0.84', 'f1 0.85', 'f3 0.85']\n",
      "joint_a   : ['precision 0.67', 'recall 0.65', 'f1 0.66', 'f3 0.65']\n",
      "no_filter : ['precision 0.60', 'recall 0.60', 'f1 0.60', 'f3 0.60']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    }
   ],
   "source": [
    "results = score(q_per_filter)\n",
    "print('', flush=True)\n",
    "\n",
    "for filter_type, metrics in results.items():\n",
    "    print(f'{filter_type:10}: {[metric + \" {:2.4}\".format(str(torch.mean(values).item())) for metric, values in metrics.items()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doing joint_h\tpython: 91calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611a1e29a14049d1a3a01bc994a66bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cc3b8ef8614bfa8ab4a6db79219a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 212.05 seconds, 0.43 sentences/sec\n",
      "\tjavascript: 7calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0c163a77904f939d639e8e5d8588bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0bd8facf5846a0a28278edf507e73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.96 seconds, 1.41 sentences/sec\n",
      "\tc: 0\tcpp: 0\tjava: 0\n",
      "doing context\tpython: 45calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6ed26baa894daabf4704b3787b1109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affea622e83b4115934c1136223ebbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 81.52 seconds, 0.55 sentences/sec\n",
      "\tjavascript: 3calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beab949bd9714717b412c422fe8182ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed81d2c85e0949f5a07205dc6a7576b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.05 seconds, 0.59 sentences/sec\n",
      "\tc: 0\tcpp: 1calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cfa377c9dc4efa8e52a0724b6cd6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227b059ee5fc4e80b1cf37c25c58384a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.45 seconds, 0.18 sentences/sec\n",
      "\tjava: 2calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4c6c26bbd54714ac2ab88c43da5a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bf4539fcbc4426a71f10b8f7b5a39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 15.02 seconds, 0.13 sentences/sec\n",
      "\n",
      "doing feature\tpython: 20calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a59dbf15ebd422597d80b99e4e695a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deee07b5728d45c588547e3fea08238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 96.80 seconds, 0.21 sentences/sec\n",
      "\tjavascript: 3calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08591aefe94c40008e57a8b38e51109f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219743c1e5514f549c65a67cca5a3edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.89 seconds, 0.51 sentences/sec\n",
      "\tc: 0\tcpp: 0\tjava: 0\n",
      "doing joint_a\tpython: 37calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7bbffcefbf449a9528ff7fe3a430a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e4b150a0184df38871c3608ac8cb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 51.56 seconds, 0.72 sentences/sec\n",
      "\tjavascript: 10calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53768c1591764fb891b8fef37bc2d15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe698ece9264f9e9de83625c6d12810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 16.51 seconds, 0.61 sentences/sec\n",
      "\tc: 0\tcpp: 0\tjava: 0\n",
      "doing no_filter\tpython: 42calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d1f64bbc514f0f87478eed8837e747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b73a6baf0240b4b8c67bd0745da3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 100.28 seconds, 0.42 sentences/sec\n",
      "\tjavascript: 16calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215a61f415a44d1da371759c4f2556cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41196bb5ab94703b23f133e095d88da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n",
      "Warning: Empty reference sentence detected; setting precision and recall to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 20.47 seconds, 0.78 sentences/sec\n",
      "\tc: 0\tcpp: 0\tjava: 2calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c731694b60a44198fc92d3979740dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d7622e850847cf9347e7bf88b94d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.96 seconds, 0.40 sentences/sec\n",
      "\n",
      "joint_h   : ['precision 0.82', 'recall 0.82', 'f1 0.82', 'f3 0.82']\n",
      "context   : ['precision 0.85', 'recall 0.85', 'f1 0.85', 'f3 0.85']\n",
      "feature   : ['precision 0.94', 'recall 0.94', 'f1 0.94', 'f3 0.94']\n",
      "joint_a   : ['precision 0.89', 'recall 0.88', 'f1 0.88', 'f3 0.88']\n",
      "no_filter : ['precision 0.76', 'recall 0.77', 'f1 0.76', 'f3 0.76']\n"
     ]
    }
   ],
   "source": [
    "results = score(q_per_filter, use_sources=True)\n",
    "print('', flush=True)\n",
    "\n",
    "for filter_type, metrics in results.items():\n",
    "    print(f'{filter_type:10}: {[metric + \" {:2.4}\".format(str(torch.mean(values).item())) for metric, values in metrics.items()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    " \n",
    "- 387 with ground truth\n",
    "- 281 supported by CodeBERTScore (others are things like html: 81, php: 48, latex: 35, css: 27 invocations)\n",
    "    (could consider including html and php as `javascript`, which is supported by CodeBERTScore due to similar syntax)\n",
    "\n",
    "CodeBERTScore first constructs embeddings, then computes similarity. Embeddings can be constructed together with the `prefix`, but the `prefix` is excluded from the similarity computation (somehow). \n",
    "\n",
    "- without prefix for embedding computation\n",
    "```\n",
    "joint_a   : ['precision 0.67', 'recall 0.65', 'f1 0.66', 'f3 0.65']\n",
    "joint_h   : ['precision 0.75', 'recall 0.74', 'f1 0.74', 'f3 0.74']\n",
    "codeberta : ['precision 0.77', 'recall 0.76', 'f1 0.77', 'f3 0.76']\n",
    "feature   : ['precision 0.86', 'recall 0.84', 'f1 0.85', 'f3 0.85']\n",
    "no_filter : ['precision 0.60', 'recall 0.60', 'f1 0.60', 'f3 0.60']\n",
    "```\n",
    "\n",
    "- with prefix for embedding computation \n",
    "```\n",
    "joint_a   : ['precision 0.89', 'recall 0.88', 'f1 0.88', 'f3 0.88']\n",
    "joint_h   : ['precision 0.82', 'recall 0.82', 'f1 0.82', 'f3 0.82']\n",
    "codeberta : ['precision 0.85', 'recall 0.85', 'f1 0.85', 'f3 0.85']\n",
    "feature   : ['precision 0.94', 'recall 0.94', 'f1 0.94', 'f3 0.94']\n",
    "no_filter : ['precision 0.76', 'recall 0.77', 'f1 0.76', 'f3 0.76']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
